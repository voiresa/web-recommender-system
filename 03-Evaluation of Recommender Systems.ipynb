{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3e29377",
   "metadata": {},
   "source": [
    "# Evaluation of Recommender Systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5370dbb",
   "metadata": {},
   "source": [
    "Note: this lab has been tested with Python 3.10. We recommend using the same Python version if there are problems with libraries used in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9d2eb054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data generated in W6 Lab or the provided data splits (see Absalon, W7 Lab)\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise import KNNWithMeans\n",
    "from surprise.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from surprise import accuracy\n",
    "from surprise import SVD\n",
    "\n",
    "train_data = pd.read_pickle(\"train_dataframe.pkl\")\n",
    "test_data = pd.read_pickle(\"test_dataframe.pkl\")\n",
    "top_items = pd.read_csv('top_items.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ee73231",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "Specifically,\n",
    "let us evaluate all your recommender models from Week 7 on the preprocessed\n",
    "test data split studied in Week 6. You need to:\n",
    "• Measure the error of the system’s predicted likelihood of rating for the\n",
    "items (Root Mean Square Error, RMSE)4\n",
    ".\n",
    "• Discuss the limitations of this metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "62b19611",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "training_matrix = Dataset.load_from_df(train_data[['user_id', 'item_id', 'rating']], reader)\n",
    "trainset = training_matrix.build_full_trainset()\n",
    "unobserved_ratings = trainset.build_anti_testset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d263f2",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "622b713b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.5344\n",
      "For KNN model, the RMSE: 0.5343510804831747\n"
     ]
    }
   ],
   "source": [
    "sim_options = {'name': 'pearson',\n",
    "               'user_based': True\n",
    "               }\n",
    "algo_knn = KNNWithMeans(k= 6, \n",
    "                    random_state=0,\n",
    "                    sim_options= sim_options,\n",
    "                    verbose=False)\n",
    "\n",
    "trainset = training_matrix.build_full_trainset()# includes the entire dataset for training\n",
    "algo_knn.fit(trainset)\n",
    "\n",
    "pred_KNN = algo_knn.test(unobserved_ratings)\n",
    "rmse_score_knn = accuracy.rmse(pred_KNN)\n",
    "\n",
    "print(f\"For KNN model, the RMSE: {rmse_score_knn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fc71b2",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9df06c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.3438\n",
      "For SVD model, the RMSE: 0.3438193128761712\n"
     ]
    }
   ],
   "source": [
    "algo_SVD = SVD(n_factors=30, n_epochs=20, random_state=0)\n",
    "algo_SVD.fit(trainset)\n",
    "\n",
    "pred_SVD = algo_SVD.test(unobserved_ratings)\n",
    "rmse_score_svd = accuracy.rmse(pred_SVD)\n",
    "\n",
    "print(f\"For SVD model, the RMSE: {rmse_score_svd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bb3a87",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "Now, we are interested not in whether the system properly predicts the rating\n",
    "of these items, but rather whether the system gives the best recommendations\n",
    "for each user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab54e73",
   "metadata": {},
   "source": [
    "To evaluate this, generate the top-k (with k = 10) recommendation\n",
    "for each test user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "39b227ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general method for top-k recommendations\n",
    "from collections import defaultdict\n",
    "from surprise.prediction_algorithms.predictions import Prediction\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "\n",
    "def get_top_k(predictions: List[Prediction], \n",
    "              k: int) -> Dict[str, List]:\n",
    "    \"\"\"Compute the top-K recommendation for each user from a set of predictions.\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        k(int): The number of recommendation to output for each user.\n",
    "    Returns:\n",
    "        A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "    topk = defaultdict(list)\n",
    "    \n",
    "    for pred in predictions:\n",
    "        uid = pred.uid \n",
    "        iid = pred.iid \n",
    "        est = pred.est \n",
    "        topk[uid].append((iid, est))\n",
    "    \n",
    "    # Sort the predictions\n",
    "    for uid, user_ratings in topk.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        topk[uid] = user_ratings[:k]\n",
    "    return topk\n",
    "\n",
    "top10_knn = get_top_k(pred_KNN, k=10)\n",
    "top10_svd = get_top_k(pred_SVD, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bf3679",
   "metadata": {},
   "source": [
    "Based on the top-k recommendation list generated for\n",
    "each user, and using the test data split\n",
    ", compute:\n",
    "• Hit rate, averaged across users.\n",
    "• Precision@k, averaged across users\n",
    "• Mean Average Precision (MAP@k)\n",
    "• Mean Reciprocal Rank (MRR@k)\n",
    "• Coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2428af",
   "metadata": {},
   "source": [
    "You need to convert 4 and 5 point ratings in the test split to binary labels, i.e., ratings\n",
    "≤ 3 are mapped to 0 and ratings ≥ 4 are mapped to 1. For example, with Hit rate, if a user\n",
    "gave a rating ≥ 4 to one of the top-k items we recommended (i.e., the item from the test split\n",
    "is among our recommendations), then we consider that as a hit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5ebfc810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using 3 as a threshold\n",
    "test_data['new_label'] = test_data['rating'].apply(lambda x: 1 if x >= 3 else 0)\n",
    "#test_data[test_data['new_label'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "79043872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import (absolute_import, division, print_function, unicode_literals)\n",
    "from collections import defaultdict\n",
    "from surprise import Dataset\n",
    "\n",
    "\n",
    "def precision_at_k(top_k: Dict[str, List[str]], df_test: pd.DataFrame, k: int) -> Dict[str, float]:\n",
    "    \"\"\"Compute precision at k for each user\n",
    "    Args:\n",
    "        top_k: A dictionary where keys are user ids (str) and values are lists of (item_id, rating_estimation) tuples.\n",
    "        df_test: Pandas DataFrame containing user-item ratings in the test split.\n",
    "        k: The number of recommendations to output for each user.\n",
    "    Returns:\n",
    "        A dictionary where keys are user ids (str) and values are P@k (float) for each user.\n",
    "    \"\"\"\n",
    "    \n",
    "    precisions = defaultdict(float)\n",
    "    \n",
    "    # Only consider relevant items (rating ≥ 3.0)\n",
    "    relevant_items = df_test[df_test['new_label'] == 1].groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "    \n",
    "    for user, recommended_items in top_k.items():\n",
    "        recommended_set = {item for item, _ in recommended_items[:k]}  # Take top-k items\n",
    "        \n",
    "        if user in relevant_items:\n",
    "            num_relevant_at_k = len(recommended_set & relevant_items.get(user, set()))  # Intersection count\n",
    "            if k > 0:  # Avoid division by zero\n",
    "                precisions[user] = round(num_relevant_at_k / min(len(recommended_items), k), 3)  # Compute Precision@k\n",
    "\n",
    "    return precisions\n",
    "\n",
    "\n",
    "\n",
    "def mean_average_precision(top_k: Dict[str, List[str]], df_test: pd.DataFrame, k: int) -> float:\n",
    "    \"\"\"Compute mean average precision (MAP@k)\n",
    "    Args:\n",
    "        top_k: A dictionary where keys are user ids (str) and values are lists of (item_id, rating_estimation) tuples.\n",
    "        df_test: Pandas DataFrame containing user-item ratings in the test split.\n",
    "        k: The number of recommendations to output for each user.\n",
    "    Returns:\n",
    "        MAP@k (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    average_precision_users = []\n",
    "    \n",
    "    # Get relevant items per user\n",
    "    relevant_items = df_test[df_test['new_label'] == 1].groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "\n",
    "    for user, recommended_items in top_k.items():\n",
    "        relevant_set = relevant_items.get(user, set())  # Get relevant items, default to empty set\n",
    "        \n",
    "        num_relevant = 0\n",
    "        precision_sum = 0.0\n",
    "        \n",
    "        for i, (item, _) in enumerate(recommended_items[:k]):  # Iterate over top-K items\n",
    "            if item in relevant_set:\n",
    "                num_relevant += 1\n",
    "                precision_sum += num_relevant / (i + 1)  # Precision at each relevant item\n",
    "\n",
    "        # Avoid division by zero\n",
    "        avg_precision = precision_sum / min(len(recommended_items), k) if num_relevant > 0 else 0\n",
    "        average_precision_users.append(avg_precision)\n",
    "\n",
    "    return np.mean(average_precision_users) if average_precision_users else 0.0\n",
    "\n",
    "\n",
    "def mean_reciprocal_rank(top_k: Dict[str, List[str]], df_test: pd.DataFrame, k: int) -> float:\n",
    "    \"\"\"Compute mean reciprocal rank (MRR@k)\n",
    "    Args:\n",
    "        top_k: A dictionary where keys are user ids (str) and values are lists of (item_id, rating_estimation) tuples.\n",
    "        df_test: Pandas DataFrame containing user-item ratings in the test split.\n",
    "        k: The number of recommendations to output for each user.\n",
    "    Returns:\n",
    "        MRR@k (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    # Get relevant items per user\n",
    "    relevant_items = df_test[df_test['new_label'] == 1].groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "\n",
    "    for user, recommended_items in top_k.items():\n",
    "        relevant_set = relevant_items.get(user, set())  # Get relevant items, default to empty set\n",
    "        found_relevant = False\n",
    "        \n",
    "        for i, (item, _) in enumerate(recommended_items[:k]):  # Iterate over top-K items\n",
    "            if item in relevant_set:  # Find first relevant item\n",
    "                reciprocal_ranks.append(1 / (i + 1))\n",
    "                found_relevant = True\n",
    "                break  # Stop after first relevant item\n",
    "\n",
    "        if not found_relevant:\n",
    "            reciprocal_ranks.append(0)  # Assign 0 if no relevant item is found\n",
    "\n",
    "    return np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "\n",
    "\n",
    "\n",
    "def hit_rate(top_k: Dict[str, List[str]],\n",
    "             df_test: pd.DataFrame) -> float:\n",
    "    \"\"\"Compute the hit rate\n",
    "    Args:\n",
    "        top_k: A dictionary where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n (output of get_top_k())\n",
    "        df_test: Pandas DataFrame containing user-item ratings in \n",
    "            the test split.\n",
    "    Returns:\n",
    "        The average hit rate\n",
    "    \"\"\"\n",
    "\n",
    "    hits = 0\n",
    "    # Get relevant items per user\n",
    "    relevant_items = df_test[df_test['new_label'] == 1].groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "    total_users = len(df_test[df_test['new_label'] == 1]['user_id'].unique())\n",
    "\n",
    "    for user, recommended_items in top_k.items():\n",
    "        recommended_set = {item for item, _ in recommended_items}  # Extract recommended item IDs\n",
    "        if user in relevant_items:\n",
    "            if recommended_set & relevant_items[user]:  # Check if there is any intersection\n",
    "                hits += 1  \n",
    "\n",
    "    return round(hits / total_users, 3) if total_users > 0 else 0.0\n",
    "\n",
    "\n",
    "\n",
    "def coverage(top_k: Dict[str, List[str]], df_test: pd.DataFrame, df_train: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Compute catalog coverage.\n",
    "\n",
    "    Args:\n",
    "        top_k: A dictionary where keys are user (raw) ids and values are lists of tuples:\n",
    "               [(raw item id, rating estimation), ...] (output of get_top_k()).\n",
    "        df_test: Pandas DataFrame containing the training data (user-item interactions).\n",
    "\n",
    "    Returns:\n",
    "        Coverage as a float (rounded to 3 decimals).\n",
    "    \"\"\"\n",
    "    if not top_k:\n",
    "        return 0.0  # No recommendations made\n",
    "\n",
    "    recommended_items = {item for recommendations in top_k.values() for item, _ in recommendations}\n",
    "    all_items = set(df_train[\"item_id\"].unique()) | set(df_test[\"item_id\"].unique()) \n",
    "\n",
    "    coverage_score = len(recommended_items) / len(all_items) if all_items else 0\n",
    "\n",
    "    return round(coverage_score, 3)  # Round to 3 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c6e670",
   "metadata": {},
   "source": [
    "### Evaluate KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "82cc7dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for KNN CF:\n",
      "Averaged P@10: 0.012\n",
      "MAP@10: 0.002\n",
      "MRR@10: 0.019\n",
      "Hit rate@10: 0.111\n",
      "Coverage@10: 0.807\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Metrics for KNN CF:\")\n",
    "# PRECISION\n",
    "precisions_nb = precision_at_k(top10_knn, test_data, k=10)\n",
    "print(\"Averaged P@10: {:.3f}\".format(sum(prec for prec in precisions_nb.values()) / len(precisions_nb)))\n",
    "# MAP \n",
    "map_nb = mean_average_precision(top10_knn, test_data, k=10)\n",
    "print(\"MAP@10: {:.3f}\".format(map_nb))\n",
    "# MRR\n",
    "mrr_nb = mean_reciprocal_rank(top10_knn, test_data, k=10)\n",
    "print(\"MRR@10: {:.3f}\".format(mrr_nb))\n",
    "# hit rate\n",
    "hit_r = hit_rate(top10_knn, test_data)\n",
    "print(\"Hit rate@10: {:.3f}\".format(hit_r))\n",
    "# coverage\n",
    "cover = coverage(top10_knn, test_data, train_data)\n",
    "print(\"Coverage@10: {:.3f}\".format(cover))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf0a092",
   "metadata": {},
   "source": [
    "### Evaluate SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b400714a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for SVD CF:\n",
      "Averaged P@10: 0.012\n",
      "MAP@10: 0.002\n",
      "MRR@10: 0.020\n",
      "Hit rate@10: 0.115\n",
      "Coverage@10: 0.255\n"
     ]
    }
   ],
   "source": [
    "print(\"Metrics for SVD CF:\")\n",
    "# PRECISION\n",
    "precisions_nb = precision_at_k(top10_svd, test_data, k=10)\n",
    "print(\"Averaged P@10: {:.3f}\".format(sum(prec for prec in precisions_nb.values()) / len(precisions_nb)))\n",
    "# MAP \n",
    "map_nb = mean_average_precision(top10_svd, test_data, k=10)\n",
    "print(\"MAP@10: {:.3f}\".format(map_nb))\n",
    "# MRR\n",
    "mrr_nb = mean_reciprocal_rank(top10_svd, test_data, k=10)\n",
    "print(\"MRR@10: {:.3f}\".format(mrr_nb))\n",
    "# hit rate\n",
    "hit_r = hit_rate(top10_svd, test_data)\n",
    "print(\"Hit rate@10: {:.3f}\".format(hit_r))\n",
    "# coverage\n",
    "cover = coverage(top10_svd, test_data, train_data)\n",
    "print(\"Coverage@10: {:.3f}\".format(cover))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "00e478f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preserve result for KNN\n",
    "prediction_KNN = {}\n",
    "for prediction in pred_KNN:\n",
    "    uid = prediction.uid\n",
    "    iid = prediction.iid\n",
    "    est = prediction.est\n",
    "    if uid not in prediction_KNN:\n",
    "        prediction_KNN[uid] = {}\n",
    "    \n",
    "    prediction_KNN[uid][iid] = est\n",
    "import pickle\n",
    "\n",
    "with open('prediction_KNN.pickle', 'wb') as file:\n",
    "    pickle.dump(prediction_KNN, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4bca79",
   "metadata": {},
   "source": [
    "### Evalute Baseline TopPop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2444d532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for TopPop:\n",
      "Averaged P@10: 0.005\n",
      "MAP@10: 0.000\n",
      "MRR@10: 0.005\n",
      "Hit rate@10: 0.047\n",
      "Coverage@10: 0.019\n"
     ]
    }
   ],
   "source": [
    "top_items_list = list(zip(top_items['item_id'], top_items['avg_rating']))\n",
    "top10_toppop = defaultdict(list)\n",
    "\n",
    "for user in prediction_KNN:\n",
    "    top10_toppop[user] = top_items_list\n",
    "\n",
    "print(\"Metrics for TopPop:\")\n",
    "# PRECISION\n",
    "precisions_nb = precision_at_k(top10_toppop, test_data, k=10)\n",
    "print(\"Averaged P@10: {:.3f}\".format(sum(prec for prec in precisions_nb.values()) / len(precisions_nb)))\n",
    "# MAP \n",
    "map_nb = mean_average_precision(top10_toppop, test_data, k=10)\n",
    "print(\"MAP@10: {:.3f}\".format(map_nb))\n",
    "# MRR\n",
    "mrr_nb = mean_reciprocal_rank(top10_toppop, test_data, k=10)\n",
    "print(\"MRR@10: {:.3f}\".format(mrr_nb))\n",
    "# hit rate\n",
    "hit_r = hit_rate(top10_toppop, test_data)\n",
    "print(\"Hit rate@10: {:.3f}\".format(hit_r))\n",
    "# coverage\n",
    "cover = coverage(top10_toppop, test_data, train_data)\n",
    "print(\"Coverage@10: {:.3f}\".format(cover))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3263722c",
   "metadata": {},
   "source": [
    "### long tail analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c73248",
   "metadata": {},
   "source": [
    "top20% and last20% user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cd6a8efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Hit Rate - Top 20%: 0.050, Last 20%: 0.181\n",
      "SVD Hit Rate - Top 20%: 0.100, Last 20%: 0.181\n",
      "TopPop Hit Rate - Top 20%: 0.033, Last 20%: 0.069\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "def hit_rate(top_k: Dict[str, List[tuple]], df_test: pd.DataFrame) -> float:\n",
    "    hits = 0\n",
    "    relevant_items = df_test[df_test['new_label'] == 1].groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "    total_users = len(relevant_items)\n",
    "\n",
    "    for user, recommended_items in top_k.items():\n",
    "        if user in relevant_items:\n",
    "            recommended_set = {item for item, _ in recommended_items}\n",
    "            if recommended_set & relevant_items[user]:\n",
    "                hits += 1\n",
    "\n",
    "    return round(hits / total_users, 3) if total_users > 0 else 0.0\n",
    "\n",
    "def top_and_last_20_hit_rate(top_k: Dict[str, List[tuple]], df_test: pd.DataFrame, df_train: pd.DataFrame) -> Tuple[float, float]:\n",
    "    user_interaction_counts = df_train.groupby('user_id').size().sort_values(ascending=False)\n",
    "    num_users = len(user_interaction_counts)\n",
    "    top_20_users = set(user_interaction_counts.head(int(0.2 * num_users)).index)\n",
    "    last_20_users = set(user_interaction_counts.tail(int(0.2 * num_users)).index)\n",
    "\n",
    "    df_test_top_20 = df_test[df_test['user_id'].isin(top_20_users)]\n",
    "    df_test_last_20 = df_test[df_test['user_id'].isin(last_20_users)]\n",
    "\n",
    "    top_20_hr = hit_rate(top_k, df_test_top_20)\n",
    "    last_20_hr = hit_rate(top_k, df_test_last_20)\n",
    "\n",
    "    return top_20_hr, last_20_hr\n",
    "\n",
    "top20_hit_knn, last20_hit_knn = top_and_last_20_hit_rate(top10_knn, test_data, train_data)\n",
    "top20_hit_svd, last20_hit_svd = top_and_last_20_hit_rate(top10_svd, test_data, train_data)\n",
    "top20_hit_toppop, last20_hit_toppop = top_and_last_20_hit_rate(top10_toppop, test_data, train_data)\n",
    "\n",
    "print(\"KNN Hit Rate - Top 20%: {:.3f}, Last 20%: {:.3f}\".format(top20_hit_knn, last20_hit_knn))\n",
    "print(\"SVD Hit Rate - Top 20%: {:.3f}, Last 20%: {:.3f}\".format(top20_hit_svd, last20_hit_svd))\n",
    "print(\"TopPop Hit Rate - Top 20%: {:.3f}, Last 20%: {:.3f}\".format(top20_hit_toppop, last20_hit_toppop))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77245a74",
   "metadata": {},
   "source": [
    "top 20% and last 20% of items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "de4b1fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN coverage - Top 20%: 0.99, Tail 20%: 0.564\n",
      "SVD coverage - Top 20%: 0.277, Tail 20%: 0.188\n",
      "TopPop coverage - Top 20%: 0.01, Tail 20%: 0.0\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Set, Tuple\n",
    "def coverage(top_k: Dict[str, List[str]], relevant_items: Set[str]) -> float:\n",
    "\n",
    "    recommended_items = {item for recs in top_k.values() for item, _ in recs}\n",
    "    matched = recommended_items & relevant_items\n",
    "    return round(len(matched)/len(relevant_items), 3) if relevant_items else 0\n",
    "\n",
    "def get_item_groups(df_train: pd.DataFrame) -> Tuple[Set[str], Set[str]]:\n",
    "\n",
    "    item_counts = df_train['item_id'].value_counts()\n",
    "    split_idx = int(len(item_counts) * 0.2)\n",
    "    return set(item_counts.head(split_idx).index), set(item_counts.tail(split_idx).index)\n",
    "\n",
    "top_items, tail_items = get_item_groups(train_data)\n",
    "\n",
    "# Calculate coverage for different groups\n",
    "def calculate_group_coverage(top_k: Dict[str, List[str]], items: Set[str]) -> float:\n",
    "    return coverage(top_k, items)\n",
    "\n",
    "\n",
    "\n",
    "top20_cov_knn = calculate_group_coverage(top10_knn, top_items)\n",
    "last20_cov_knn = calculate_group_coverage(top10_knn, tail_items)\n",
    "print(f\"KNN coverage - Top 20%: {top20_cov_knn}, Tail 20%: {last20_cov_knn}\")\n",
    "\n",
    "top20_cov_svd = calculate_group_coverage(top10_svd, top_items)\n",
    "last20_cov_svd = calculate_group_coverage(top10_svd, tail_items)\n",
    "print(f\"SVD coverage - Top 20%: {top20_cov_svd}, Tail 20%: {last20_cov_svd}\")\n",
    "\n",
    "top20_cov_toppop = calculate_group_coverage(top10_toppop, top_items)\n",
    "last20_cov_toppop = calculate_group_coverage(top10_toppop, tail_items)\n",
    "print(f\"TopPop coverage - Top 20%: {top20_cov_toppop}, Tail 20%: {last20_cov_toppop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c8e5f",
   "metadata": {},
   "source": [
    "### Error analysis for the neighborhood-based CF:\n",
    "•Select two users from the training set, one with high RR and one with\n",
    "low RR. These are your reference users. Retrieve the 10 nearest neigh-\n",
    "bours of each reference user. Print their rating history and analyse their\n",
    "predictions. How much overlap is there between the rating history of the\n",
    "reference users and their neighbours?\n",
    "•For those users or items that your model performs poorly on (RR ≤0.05),\n",
    "discuss the potential reasons behind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ebc651ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High RR User: AF2MGAQBAT3E4XQC7NNAQFYG4MIQ\n",
      "Low RR User: AEHWHU26Z2VEZQY3IEEW5HBIGX5Q\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from surprise import accuracy\n",
    "import random\n",
    "\n",
    "def calculate_rr(user_id, test_data, predictions, max_rank=100):\n",
    "    relevant_items = set(test_data[test_data['user_id'] == user_id]['item_id'])\n",
    "    user_predictions = sorted(\n",
    "        [pred for pred in predictions if pred.uid == user_id],\n",
    "        key=lambda x: x.est,\n",
    "        reverse=True\n",
    "    )[:max_rank]  \n",
    "    for rank, pred in enumerate(user_predictions, 1):\n",
    "        if pred.iid in relevant_items:\n",
    "            return 1 / rank\n",
    "    return 0\n",
    "\n",
    "sampled_users = random.sample(list(set(pred.uid for pred in pred_KNN)), 800)\n",
    "user_rr = {user: calculate_rr(user, test_data, pred_KNN) for user in sampled_users}\n",
    "\n",
    "high_rr_user = max(user_rr, key=user_rr.get)\n",
    "low_rr_user = min(user_rr, key=user_rr.get)\n",
    "print(f\"High RR User: {high_rr_user}\")\n",
    "print(f\"Low RR User: {low_rr_user}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "511e8be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting neighbors...\n",
      "Printing rating histories...\n",
      "Rating History for User AF2MGAQBAT3E4XQC7NNAQFYG4MIQ (top 10 items):\n",
      "Item B000SHQ1QC: 4.0\n",
      "Rating History for Neighbors (top 5 items each):\n",
      "Neighbor AE23LDQTB7L76AP6E6WPBFVYL5DA:\n",
      "Item B005M0MUQK: 4.0\n",
      "Item B007T8CUNG: 5.0\n",
      "Item B015IJIO5U: 5.0\n",
      "Item B09NLV5LBK: 5.0\n",
      "Item B0BSR996X8: 5.0\n",
      "Neighbor AE23ZFVUOMPKR57BVSWXV34QLMVA:\n",
      "Item B000NGVQKO: 5.0\n",
      "Item B005PGGU9O: 3.0\n",
      "Item B00CPLODUU: 5.0\n",
      "Item B00RX5HQS4: 5.0\n",
      "Item B00VSYN25M: 5.0\n",
      "Neighbor AE2BV2H57ERXAPW7SOAXFLWA2S2Q:\n",
      "Item B00EM5UOE6: 4.0\n",
      "Item B00JL5I61A: 5.0\n",
      "Item B015IJIO5U: 3.0\n",
      "Item B06Y2W66F5: 1.0\n",
      "Item B07T3GZVMD: 5.0\n",
      "Neighbor AE2EQIUBKQCVSP55MNF2SQIASN2Q:\n",
      "Item B003AYNHGW: 5.0\n",
      "Item B00H35YIJE: 5.0\n",
      "Item B00JL5I3AO: 5.0\n",
      "Item B01DE4DTAG: 5.0\n",
      "Item B08SJY4T7K: 5.0\n",
      "Neighbor AE2NWSTL7JJJWOCBKZCZF6KDQIZQ:\n",
      "Item B015WUVA5G: 5.0\n",
      "Item B07226MCY6: 4.0\n",
      "Item B0BRS6V8G4: 5.0\n",
      "Neighbor AE2OQ55HLV5XO54DWLE4PB5XUNPA:\n",
      "Item B0002E3FBA: 5.0\n",
      "Item B079P9LDHN: 5.0\n",
      "Item B07B16JL73: 4.0\n",
      "Item B07D4QCBX4: 5.0\n",
      "Item B07GFH8NTB: 5.0\n",
      "Neighbor AE2TQUALUTZLMEWBULG5K65EOQSQ:\n",
      "Item B00CPLODUU: 5.0\n",
      "Item B00IFOTSJW: 5.0\n",
      "Item B07CBT4SYD: 5.0\n",
      "Item B08JMQR2JK: 5.0\n",
      "Item B08R5GM6YB: 5.0\n",
      "Neighbor AE2W7OBJ45J4J52CGH7OFWHQZJQA:\n",
      "Item B0002D01KO: 3.0\n",
      "Item B005H2007E: 5.0\n",
      "Item B01DE4DTAG: 5.0\n",
      "Item B077GK1J7N: 5.0\n",
      "Item B079P9LDHN: 5.0\n",
      "Neighbor AE37RAW77LNOTEDKMDKGXSGQHD5Q:\n",
      "Item B0002D0MI0: 5.0\n",
      "Item B000TGSM6E: 5.0\n",
      "Item B0010SHU18: 4.0\n",
      "Item B00646MZHK: 4.0\n",
      "Item B00HZH4D04: 5.0\n",
      "Neighbor AE3C2M62HHDRFDW6E2XADFQUXCGQ:\n",
      "Item B00HZH4D04: 5.0\n",
      "Item B015X3CXXA: 5.0\n",
      "Item B07S19XSPV: 5.0\n",
      "Item B07TDNWYSB: 5.0\n",
      "Item B091MZPYBQ: 5.0\n",
      "Rating History for User AEHWHU26Z2VEZQY3IEEW5HBIGX5Q (top 10 items):\n",
      "Item B000RNB720: 5.0\n",
      "Item B002G1UT5C: 4.0\n",
      "Item B004ZKIHVU: 3.0\n",
      "Item B00DY1F2CS: 5.0\n",
      "Item B00H35YIJE: 2.0\n",
      "Item B00IFOTSJW: 5.0\n",
      "Item B00OYUSHJS: 4.0\n",
      "Item B01DE4DEGK: 5.0\n",
      "Item B01DE4DPSC: 3.0\n",
      "Item B01K1EJQ9U: 4.0\n",
      "Rating History for Neighbors (top 5 items each):\n",
      "Neighbor AE3HJ6CT7RAMP5XNGLYPEGDEMJJQ:\n",
      "Item B0002E2G5Q: 5.0\n",
      "Item B00IZA1GI2: 5.0\n",
      "Item B015HG7HMA: 5.0\n",
      "Item B015JTEBN8: 3.0\n",
      "Item B0192JRA2A: 5.0\n",
      "Neighbor AE6BYVUHZCRER7JBO5CFHJEOHAVQ:\n",
      "Item B000P5OULA: 5.0\n",
      "Item B002G1UT5C: 3.0\n",
      "Item B009DAHPLU: 5.0\n",
      "Item B00D35CNL8: 4.0\n",
      "Item B06XB3FQKB: 4.0\n",
      "Neighbor AE7P3HIBI3UDLCJLUPUZQWYVPEWA:\n",
      "Item B000NGVQKO: 4.0\n",
      "Item B003AYNHGW: 5.0\n",
      "Item B00H35YIJE: 4.0\n",
      "Item B074WXM13T: 5.0\n",
      "Item B07B4S63NN: 4.0\n",
      "Neighbor AEPN2RD5DNLVBU4OQPDDZLM5EAMA:\n",
      "Item B004XNK7AI: 5.0\n",
      "Item B00CGFRJ2Y: 5.0\n",
      "Item B00CPLODUU: 4.0\n",
      "Item B074WXM13T: 5.0\n",
      "Item B07FYPBL6K: 4.0\n",
      "Neighbor AERJUD4NREPMW7VM26CSV32KJZJQ:\n",
      "Item B00646MZHK: 5.0\n",
      "Item B00BESRB8Q: 5.0\n",
      "Item B00CGFRJ2Y: 5.0\n",
      "Item B00CPLODUU: 4.0\n",
      "Item B00H4PEMM6: 4.0\n",
      "Neighbor AEW2O7D7NTWRZ6Z2MRTRSFN3HKEQ:\n",
      "Item B0035LCFRW: 1.0\n",
      "Item B00646MZHK: 3.0\n",
      "Item B008L3SXE8: 1.0\n",
      "Item B00IFOTSJW: 4.0\n",
      "Item B01DE4DTAG: 5.0\n",
      "Neighbor AF5TD7CMXZ6FGOR5EY4IZPFTROGA:\n",
      "Item B003AYNHGW: 5.0\n",
      "Item B004XNK7AI: 2.0\n",
      "Item B008BPI2OW: 4.0\n",
      "Item B00NARHNCS: 3.0\n",
      "Item B015HG7HMA: 4.0\n",
      "Neighbor AFBIB5XMB27ZIUM3YDTESOKSLLQA:\n",
      "Item 1423414357: 4.0\n",
      "Item B0002D0CEO: 5.0\n",
      "Item B00646MZHK: 5.0\n",
      "Item B00H35YIJE: 2.0\n",
      "Item B00HU260SW: 5.0\n",
      "Neighbor AFI3PUWOT5VQ2RUUVOCEFT3B74BQ:\n",
      "Item B0098DFNY8: 2.0\n",
      "Item B00H35YIJE: 2.0\n",
      "Item B09VSJD5GP: 5.0\n",
      "Item B0BFKQGWZ7: 5.0\n",
      "Item B0BPJ4Q6FJ: 4.0\n",
      "Neighbor AFKBZVPC42DGIECDEAOCL57MRAQA:\n",
      "Item 1423414357: 5.0\n",
      "Item B000T9PE9E: 3.0\n",
      "Item B003AYNHGW: 5.0\n",
      "Item B00ADHKKZK: 2.0\n",
      "Item B0742RB7JK: 5.0\n",
      "Overlap rate in Rating History for User AF2MGAQBAT3E4XQC7NNAQFYG4MIQ: 0.0\n",
      "Overlap rate in Rating History for User AEHWHU26Z2VEZQY3IEEW5HBIGX5Q: 0.5\n"
     ]
    }
   ],
   "source": [
    "# 10 nearest neighbors\n",
    "def get_neighbors(user_id, algo_knn, trainset):\n",
    "    user_inner_id = trainset.to_inner_uid(user_id)\n",
    "    neighbors = algo_knn.get_neighbors(user_inner_id, k=10)\n",
    "    return [trainset.to_raw_uid(inner_id) for inner_id in neighbors]\n",
    "\n",
    "print(\"Getting neighbors...\")\n",
    "high_rr_neighbors = get_neighbors(high_rr_user, algo_knn, trainset)\n",
    "low_rr_neighbors = get_neighbors(low_rr_user, algo_knn, trainset)\n",
    "\n",
    "# Print rating history\n",
    "def print_rating_history(user_id, neighbors, trainset, max_items=10):\n",
    "    print(f\"Rating History for User {user_id} (top {max_items} items):\")\n",
    "    user_ratings = trainset.ur[trainset.to_inner_uid(user_id)][:max_items]\n",
    "    for item_inner_id, rating in user_ratings:\n",
    "        print(f\"Item {trainset.to_raw_iid(item_inner_id)}: {rating}\")\n",
    "    print(\"Rating History for Neighbors (top 5 items each):\")\n",
    "    for neighbor in neighbors:\n",
    "        neighbor_ratings = trainset.ur[trainset.to_inner_uid(neighbor)][:5]\n",
    "        print(f\"Neighbor {neighbor}:\")\n",
    "        for item_inner_id, rating in neighbor_ratings:\n",
    "            print(f\"Item {trainset.to_raw_iid(item_inner_id)}: {rating}\")\n",
    "\n",
    "print(\"Printing rating histories...\")\n",
    "print_rating_history(high_rr_user, high_rr_neighbors, trainset)\n",
    "print_rating_history(low_rr_user, low_rr_neighbors, trainset)\n",
    "\n",
    "# overlap in rating history\n",
    "def analyze_overlap(user_id, neighbors, trainset):\n",
    "    user_items = set(trainset.to_raw_iid(item_inner_id) for item_inner_id, _ in trainset.ur[trainset.to_inner_uid(user_id)])\n",
    "    neighbor_items = set()\n",
    "    for neighbor in neighbors:\n",
    "        neighbor_items.update(trainset.to_raw_iid(item_inner_id) for item_inner_id, _ in trainset.ur[trainset.to_inner_uid(neighbor)])\n",
    "    overlap = user_items.intersection(neighbor_items)\n",
    "    print(f\"Overlap rate in Rating History for User {user_id}: {len(overlap)/len(user_items)}\")\n",
    "\n",
    "analyze_overlap(high_rr_user, high_rr_neighbors, trainset)\n",
    "analyze_overlap(low_rr_user, low_rr_neighbors, trainset)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
