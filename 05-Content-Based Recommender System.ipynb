{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-Based Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: similar to the task in Week 9, for this part, you are free to only\n",
    "use a subset of the metadata file, e.g., only using items that are in the\n",
    "train/test splits or using the first n characters/words of the text content.\n",
    "You are also welcome to use additional metadata (e.g., by crawling the\n",
    "internet). Please report and justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "preprocessed_meta_file = 'preprocessed_metadata.tsv'\n",
    "df_preprocessed_meta = pd.read_csv(preprocessed_meta_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the description column of each item into a TF-IDF represen-\n",
    "tation or other numerical value, e.g., token-count based, that can represent\n",
    "the summaries. Select at least one other factor that can be used as an\n",
    "item feature, for example title. Apply the appropriate preprocessing on\n",
    "the features. These choices should also be reported and justified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('map_asin_id.pickle', 'rb') as handle:\n",
    "    map_asin_id = pickle.load(handle)\n",
    "\n",
    "map_id_asin = {v: k for k, v in map_asin_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size of description before preprocessing: 5996\n",
      "Vocabulary size of title before preprocessing: 1630\n",
      "Vocabulary size of descriptipn after preprocessing: 4470\n",
      "Vocabulary size of title after preprocessing: 1476\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "original_vocab_de = set()\n",
    "for desc in df_preprocessed_meta['description']:\n",
    "    original_vocab_de.update(word_tokenize(desc.lower()))\n",
    "print(f\"Vocabulary size of description before preprocessing: {len(original_vocab_de)}\")\n",
    "\n",
    "original_vocab_ti = set()\n",
    "for desc in df_preprocessed_meta['title']:\n",
    "    original_vocab_ti.update(word_tokenize(desc.lower()))\n",
    "print(f\"Vocabulary size of title before preprocessing: {len(original_vocab_ti)}\")\n",
    "\n",
    "def preprocess_title(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "df_preprocessed_meta['processed_title'] = df_preprocessed_meta['title'].apply(preprocess_title)\n",
    "df_preprocessed_meta['processed_description'] = df_preprocessed_meta['description'].apply(preprocess_title)\n",
    "\n",
    "processed_vocab_de = set()\n",
    "for tokens in df_preprocessed_meta['processed_description']:\n",
    "    processed_vocab_de.update(tokens)\n",
    "print(f\"Vocabulary size of descriptipn after preprocessing: {len(processed_vocab_de)}\")\n",
    "\n",
    "processed_vocab_ti = set()\n",
    "for tokens in df_preprocessed_meta['processed_title']:\n",
    "    processed_vocab_ti.update(tokens)\n",
    "print(f\"Vocabulary size of title after preprocessing: {len(processed_vocab_ti)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description TF-IDF matrix shape: (421, 4420)\n",
      "title TF-IDF matrix shape: (421, 1455)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.12066681, 0.2901492 , 0.16270751, ..., 0.21471708, 0.21471708,\n",
       "       0.21471708])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed_meta['processed_title'] = df_preprocessed_meta['processed_title'].apply(\n",
    "    lambda x: ' '.join(x) if isinstance(x, list) else x\n",
    ")\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tit_tfidf_matrix = vectorizer.fit_transform(df_preprocessed_meta['processed_title'])\n",
    "df_preprocessed_meta['processed_description'] = df_preprocessed_meta['processed_description'].apply(\n",
    "    lambda x: ' '.join(x) if isinstance(x, list) else x\n",
    ")\n",
    "\n",
    "des_tfidf_matrix = vectorizer.fit_transform(df_preprocessed_meta['processed_description'])\n",
    "\n",
    "\n",
    "print(f\"description TF-IDF matrix shape: {des_tfidf_matrix.shape}\")\n",
    "des_tfidf_matrix.data\n",
    "print(f\"title TF-IDF matrix shape: {tit_tfidf_matrix.shape}\")\n",
    "tit_tfidf_matrix.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_vectors matrix shape: (421, 5875)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.01969378, 0.02450674, 0.09562047, ..., 0.21471708, 0.21471708,\n",
       "       0.21471708])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine Representations (Concatenation)\n",
    "from scipy.sparse import hstack\n",
    "item_feature_matrix = hstack([des_tfidf_matrix, tit_tfidf_matrix])\n",
    "print(f\"item_vectors matrix shape: {item_feature_matrix.shape}\")\n",
    "item_feature_matrix.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you represent each item in a vector space, represent each user in\n",
    "the same vector space. This can be done by using a simple average of the\n",
    "items the user rated. Note that you can also try to implement better ways\n",
    "to build the user representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the user-item rating prediction for an item by using a simi-\n",
    "larity/distance metric between the user and the item representations. A\n",
    "metric such as cosine similarity or Euclidean distance can be used. Moti-\n",
    "vate your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Dict, Any, List\n",
    "from collections import defaultdict\n",
    "# Load train and test data\n",
    "train_df = pd.read_pickle(\"train_dataframe.pkl\") \n",
    "test_df = pd.read_pickle(\"test_dataframe.pkl\")\n",
    "\n",
    "# Build user ratings dictionary\n",
    "def extract_user_ratings(df: pd.DataFrame) -> Dict[str, Dict[str, float]]:\n",
    "\n",
    "    user_ratings = defaultdict(dict)\n",
    "    for _, row in df.iterrows():\n",
    "        user_ratings[row[\"user_id\"]][row[\"item_id\"]] = row[\"rating\"]\n",
    "    \n",
    "    return user_ratings\n",
    "\n",
    "user_ratings = extract_user_ratings(train_df)\n",
    "\n",
    "def compute_user_mean_ratings(user_ratings: Dict[str, Dict[str, float]]) -> Dict[str, float]:\n",
    "\n",
    "    user_mean_ratings = {}\n",
    "\n",
    "    for user_id, ratings in user_ratings.items():\n",
    "        if ratings:\n",
    "            mean_rating = sum(ratings.values()) / len(ratings)\n",
    "            user_mean_ratings[user_id] = mean_rating\n",
    "        else:\n",
    "            user_mean_ratings[user_id] = 0.0\n",
    "\n",
    "    return user_mean_ratings\n",
    "    \n",
    "def get_top_k_user_i(predictions: Dict[str, Dict[str, float]], user_id: str, k: int) -> List[Any]:\n",
    "    top_k = []\n",
    "\n",
    "    if user_id in predictions:\n",
    "        sorted_items = sorted(predictions[user_id].items(), key=lambda x: x[1], reverse=True)\n",
    "        top_k = [(asin, round(rating, 3)) for asin, rating in sorted_items[:k]]  \n",
    "\n",
    "    return top_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 recommended items for user 'AFIOMYWY7N3H6KHV3EHENDBAN7MA':\n",
      "[('B07D5W5X3Z', 3.91),\n",
      " ('B07DWLYGKH', 3.459),\n",
      " ('B098KXQJVY', 3.444),\n",
      " ('B07B16JL73', 3.43),\n",
      " ('B07DWY7R2X', 3.415)]\n",
      "Top-10 recommended items for user 'AFIOMYWY7N3H6KHV3EHENDBAN7MA':\n",
      "[('B07D5W5X3Z', 3.91),\n",
      " ('B07DWLYGKH', 3.459),\n",
      " ('B098KXQJVY', 3.444),\n",
      " ('B07B16JL73', 3.43),\n",
      " ('B07DWY7R2X', 3.415),\n",
      " ('B079Y9L1G1', 3.409),\n",
      " ('B07V46KRD8', 3.392),\n",
      " ('B06XB3FQKB', 3.385),\n",
      " ('B01C5TBX68', 3.385),\n",
      " ('B0B95V41NR', 3.369)]\n",
      "Top-20 recommended items for user 'AFIOMYWY7N3H6KHV3EHENDBAN7MA':\n",
      "[('B07D5W5X3Z', 3.91),\n",
      " ('B07DWLYGKH', 3.459),\n",
      " ('B098KXQJVY', 3.444),\n",
      " ('B07B16JL73', 3.43),\n",
      " ('B07DWY7R2X', 3.415),\n",
      " ('B079Y9L1G1', 3.409),\n",
      " ('B07V46KRD8', 3.392),\n",
      " ('B06XB3FQKB', 3.385),\n",
      " ('B01C5TBX68', 3.385),\n",
      " ('B0B95V41NR', 3.369),\n",
      " ('B07B4S63NN', 3.36),\n",
      " ('B0B2T1LF1L', 3.358),\n",
      " ('B0BZ1XQX97', 3.358),\n",
      " ('B0BKZ5F8BS', 3.343),\n",
      " ('B0064RTS0G', 3.326),\n",
      " ('B09BF8XDF4', 3.31),\n",
      " ('B09G5KDVWW', 3.294),\n",
      " ('B095XZJ99J', 3.274),\n",
      " ('B015QK3GUO', 3.269),\n",
      " ('B08VCYR3T9', 3.269)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pprint import pprint\n",
    "from typing import Dict, Any, List\n",
    "# Compute User Profile\n",
    "def compute_user_profile(user_id: str, user_ratings: Dict[str, Dict[str, float]], item_feature_matrix) -> np.ndarray:\n",
    "    if user_id not in user_ratings:\n",
    "        return np.zeros(item_feature_matrix.shape[1])  # Return a zero vector for missing users\n",
    "    # Items rated by the user\n",
    "    rated_items = user_ratings[user_id]  \n",
    "    item_vectors = []\n",
    "    weights = []\n",
    "\n",
    "    for asin, rating in rated_items.items():\n",
    "        if asin in map_asin_id:\n",
    "            item_id = map_asin_id[asin]\n",
    "            item_vectors.append(item_feature_matrix[item_id].toarray()) \n",
    "            weights.append(rating)  # Use rating as weight\n",
    "\n",
    "    if not item_vectors:\n",
    "        return np.zeros(item_feature_matrix.shape[1])  # Empty profile\n",
    "\n",
    "    item_vectors = np.vstack(item_vectors)\n",
    "    weights = np.array(weights).reshape(-1, 1)\n",
    "    user_profile = np.sum(item_vectors * weights, axis=0) / np.sum(weights)\n",
    "\n",
    "    return user_profile  \n",
    "\n",
    "\n",
    "# Compute predictions\n",
    "def compute_recommendations(user_ratings, item_feature_matrix, map_id_asin):\n",
    "    recommendations = {}\n",
    "\n",
    "    user_mean_ratings = compute_user_mean_ratings(user_ratings)\n",
    "\n",
    "    for user_id in user_ratings.keys():\n",
    "        user_profile = compute_user_profile(user_id, user_ratings, item_feature_matrix)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        similarities = cosine_similarity(user_profile.reshape(1, -1), item_feature_matrix).flatten()\n",
    "        # Get items the user has already rated\n",
    "        rated_items = set(user_ratings[user_id].keys())\n",
    "        # excluding rated items\n",
    "        recommendations[user_id] = {\n",
    "            map_id_asin[i]: float(np.clip(user_mean_ratings[user_id] + similarities[i] * 4, 1.0, 5.0)) \n",
    "            for i in range(len(similarities))\n",
    "            if map_id_asin[i] not in rated_items  # Exclude rated items\n",
    "        }\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "# Compute cosine similarity between users and items\n",
    "predictions = compute_recommendations(user_ratings, item_feature_matrix, map_id_asin)\n",
    "\n",
    "\n",
    "user_id = 'AFIOMYWY7N3H6KHV3EHENDBAN7MA' \n",
    "topk_5 = get_top_k_user_i(predictions , user_id, 5 )\n",
    "print(f\"Top-5 recommended items for user '{user_id}':\")\n",
    "pprint(topk_5)\n",
    "\n",
    "topk_10 = get_top_k_user_i(predictions , user_id, 10 )\n",
    "print(f\"Top-10 recommended items for user '{user_id}':\")\n",
    "pprint(topk_10)\n",
    "\n",
    "topk_20 = get_top_k_user_i(predictions , user_id, 20 )\n",
    "print(f\"Top-20 recommended items for user '{user_id}':\")\n",
    "pprint(topk_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report Precision@10, MAP@10, MRR@10, hit rate and coverage using\n",
    "ratings ≥4 in the test set. Compare the results with the models from\n",
    "previous weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_top_k_for_all_users(predictions: Dict[str, Dict[str, float]], k: int) -> defaultdict:\n",
    "    \n",
    "    top_k_recommendations = defaultdict(list)\n",
    "\n",
    "    for user_id, user_predictions in predictions.items():\n",
    "        sorted_items = sorted(user_predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_k_items = [(item_id, round(rating, 3)) for item_id, rating in sorted_items[:k]]\n",
    "        top_k_recommendations[user_id] = top_k_items\n",
    "\n",
    "    return top_k_recommendations\n",
    "\n",
    "k = 10\n",
    "top10_tfidf = get_top_k_for_all_users(predictions, k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['new_label'] = test_df['rating'].apply(lambda x: 1 if x >= 3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import (absolute_import, division, print_function, unicode_literals)\n",
    "from collections import defaultdict\n",
    "from surprise import Dataset\n",
    "\n",
    "\n",
    "def precision_at_k(top_k: Dict[str, List[str]], df_test: pd.DataFrame, k: int) -> Dict[str, float]:\n",
    "    \"\"\"Compute precision at k for each user\n",
    "    Args:\n",
    "        top_k: A dictionary where keys are user ids (str) and values are lists of (item_id, rating_estimation) tuples.\n",
    "        df_test: Pandas DataFrame containing user-item ratings in the test split.\n",
    "        k: The number of recommendations to output for each user.\n",
    "    Returns:\n",
    "        A dictionary where keys are user ids (str) and values are P@k (float) for each user.\n",
    "    \"\"\"\n",
    "    \n",
    "    precisions = defaultdict(float)\n",
    "    \n",
    "    # Only consider relevant items (rating ≥ 4.0)\n",
    "    relevant_items = df_test[df_test['new_label'] == 1].groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "    \n",
    "    for user, recommended_items in top_k.items():\n",
    "        recommended_set = {item for item, _ in recommended_items[:k]}  # Take top-k items\n",
    "        \n",
    "        if user in relevant_items:\n",
    "            num_relevant_at_k = len(recommended_set & relevant_items.get(user, set()))  # Intersection count\n",
    "            if k > 0:  # Avoid division by zero\n",
    "                precisions[user] = round(num_relevant_at_k / min(len(recommended_items), k), 3)  # Compute Precision@k\n",
    "\n",
    "    return precisions\n",
    "\n",
    "\n",
    "\n",
    "def mean_average_precision(top_k: Dict[str, List[str]], df_test: pd.DataFrame, k: int) -> float:\n",
    "    \"\"\"Compute mean average precision (MAP@k)\n",
    "    Args:\n",
    "        top_k: A dictionary where keys are user ids (str) and values are lists of (item_id, rating_estimation) tuples.\n",
    "        df_test: Pandas DataFrame containing user-item ratings in the test split.\n",
    "        k: The number of recommendations to output for each user.\n",
    "    Returns:\n",
    "        MAP@k (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    average_precision_users = []\n",
    "    \n",
    "    # Get relevant items per user\n",
    "    relevant_items = df_test[df_test['new_label'] == 1].groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "\n",
    "    for user, recommended_items in top_k.items():\n",
    "        relevant_set = relevant_items.get(user, set())  # Get relevant items, default to empty set\n",
    "        \n",
    "        num_relevant = 0\n",
    "        precision_sum = 0.0\n",
    "        \n",
    "        for i, (item, _) in enumerate(recommended_items[:k]):  # Iterate over top-K items\n",
    "            if item in relevant_set:\n",
    "                num_relevant += 1\n",
    "                precision_sum += num_relevant / (i + 1)  # Precision at each relevant item\n",
    "\n",
    "        # Avoid division by zero\n",
    "        avg_precision = precision_sum / min(len(recommended_items), k) if num_relevant > 0 else 0\n",
    "        average_precision_users.append(avg_precision)\n",
    "\n",
    "    return np.mean(average_precision_users) if average_precision_users else 0.0\n",
    "\n",
    "\n",
    "def mean_reciprocal_rank(top_k: Dict[str, List[str]], df_test: pd.DataFrame, k: int) -> float:\n",
    "    \"\"\"Compute mean reciprocal rank (MRR@k)\n",
    "    Args:\n",
    "        top_k: A dictionary where keys are user ids (str) and values are lists of (item_id, rating_estimation) tuples.\n",
    "        df_test: Pandas DataFrame containing user-item ratings in the test split.\n",
    "        k: The number of recommendations to output for each user.\n",
    "    Returns:\n",
    "        MRR@k (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    # Get relevant items per user\n",
    "    relevant_items = df_test[df_test['new_label'] == 1].groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "\n",
    "    for user, recommended_items in top_k.items():\n",
    "        relevant_set = relevant_items.get(user, set())  # Get relevant items, default to empty set\n",
    "        found_relevant = False\n",
    "        \n",
    "        for i, (item, _) in enumerate(recommended_items[:k]):  # Iterate over top-K items\n",
    "            if item in relevant_set:  # Find first relevant item\n",
    "                reciprocal_ranks.append(1 / (i + 1))\n",
    "                found_relevant = True\n",
    "                break  # Stop after first relevant item\n",
    "\n",
    "        if not found_relevant:\n",
    "            reciprocal_ranks.append(0)  # Assign 0 if no relevant item is found\n",
    "\n",
    "    return np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "\n",
    "\n",
    "\n",
    "def hit_rate(top_k: Dict[str, List[str]],\n",
    "             df_test: pd.DataFrame) -> float:\n",
    "    \"\"\"Compute the hit rate\n",
    "    Args:\n",
    "        top_k: A dictionary where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n (output of get_top_k())\n",
    "        df_test: Pandas DataFrame containing user-item ratings in \n",
    "            the test split.\n",
    "    Returns:\n",
    "        The average hit rate\n",
    "    \"\"\"\n",
    "\n",
    "    hits = 0\n",
    "    # Get relevant items per user\n",
    "    relevant_items = df_test[df_test['new_label'] == 1].groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "    total_users = len(df_test[df_test['new_label'] == 1]['user_id'].unique())\n",
    "\n",
    "    for user, recommended_items in top_k.items():\n",
    "        recommended_set = {item for item, _ in recommended_items}  # Extract recommended item IDs\n",
    "        if user in relevant_items:\n",
    "            if recommended_set & relevant_items[user]:  # Check if there is any intersection\n",
    "                hits += 1  \n",
    "\n",
    "    return round(hits / total_users, 3) if total_users > 0 else 0.0\n",
    "\n",
    "\n",
    "\n",
    "def coverage(top_k: Dict[str, List[str]], df_test: pd.DataFrame, df_train: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Compute catalog coverage.\n",
    "\n",
    "    Args:\n",
    "        top_k: A dictionary where keys are user (raw) ids and values are lists of tuples:\n",
    "               [(raw item id, rating estimation), ...] (output of get_top_k()).\n",
    "        df_test: Pandas DataFrame containing the training data (user-item interactions).\n",
    "\n",
    "    Returns:\n",
    "        Coverage as a float (rounded to 3 decimals).\n",
    "    \"\"\"\n",
    "    if not top_k:\n",
    "        return 0.0  # No recommendations made\n",
    "\n",
    "    recommended_items = {item for recommendations in top_k.values() for item, _ in recommendations}\n",
    "    all_items = set(df_train[\"item_id\"].unique()) | set(df_test[\"item_id\"].unique()) \n",
    "\n",
    "    coverage_score = len(recommended_items) / len(all_items) if all_items else 0\n",
    "\n",
    "    return round(coverage_score, 3)  # Round to 3 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for TF IDF:\n",
      "Averaged P@10: 0.013\n",
      "MAP@10: 0.002\n",
      "MRR@10: 0.021\n",
      "Hit rate@10: 0.132\n",
      "Coverage@10: 0.633\n"
     ]
    }
   ],
   "source": [
    "print(\"Metrics for TF IDF:\")\n",
    "# PRECISION\n",
    "precisions_nb = precision_at_k(top10_tfidf, test_df, k=10)\n",
    "print(\"Averaged P@10: {:.3f}\".format(sum(prec for prec in precisions_nb.values()) / len(precisions_nb)))\n",
    "# MAP \n",
    "map_nb = mean_average_precision(top10_tfidf, test_df, k=10)\n",
    "print(\"MAP@10: {:.3f}\".format(map_nb))\n",
    "# MRR\n",
    "mrr_nb = mean_reciprocal_rank(top10_tfidf, test_df, k=10)\n",
    "print(\"MRR@10: {:.3f}\".format(mrr_nb))\n",
    "# hit rate\n",
    "hit_r = hit_rate(top10_tfidf, test_df)\n",
    "print(\"Hit rate@10: {:.3f}\".format(hit_r))\n",
    "# coverage\n",
    "cover = coverage(top10_tfidf, test_df, train_df)\n",
    "print(\"Coverage@10: {:.3f}\".format(cover))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('prediction_TFIDF.pickle', 'wb') as file:\n",
    "    pickle.dump(predictions, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long tail analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Hit Rate - Top 20%: 0.167, Last 20%: 0.163\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "def hit_rate(top_k: Dict[str, List[tuple]], df_test: pd.DataFrame) -> float:\n",
    "    hits = 0\n",
    "    relevant_items = df_test[df_test['new_label'] == 1].groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "    total_users = len(relevant_items)\n",
    "\n",
    "    for user, recommended_items in top_k.items():\n",
    "        if user in relevant_items:\n",
    "            recommended_set = {item for item, _ in recommended_items}\n",
    "            if recommended_set & relevant_items[user]:\n",
    "                hits += 1\n",
    "\n",
    "    return round(hits / total_users, 3) if total_users > 0 else 0.0\n",
    "\n",
    "def top_and_last_20_hit_rate(top_k: Dict[str, List[tuple]], df_test: pd.DataFrame, df_train: pd.DataFrame) -> Tuple[float, float]:\n",
    "    user_interaction_counts = df_train.groupby('user_id').size().sort_values(ascending=False)\n",
    "    num_users = len(user_interaction_counts)\n",
    "    top_20_users = set(user_interaction_counts.head(int(0.2 * num_users)).index)\n",
    "    last_20_users = set(user_interaction_counts.tail(int(0.2 * num_users)).index)\n",
    "\n",
    "    df_test_top_20 = df_test[df_test['user_id'].isin(top_20_users)]\n",
    "    df_test_last_20 = df_test[df_test['user_id'].isin(last_20_users)]\n",
    "\n",
    "    top_20_hr = hit_rate(top_k, df_test_top_20)\n",
    "    last_20_hr = hit_rate(top_k, df_test_last_20)\n",
    "\n",
    "    return top_20_hr, last_20_hr\n",
    "\n",
    "top20_hit_tf, last20_hit_tf= top_and_last_20_hit_rate(top10_tfidf, test_df, train_df)\n",
    "\n",
    "print(\"TF-IDF Hit Rate - Top 20%: {:.3f}, Last 20%: {:.3f}\".format(top20_hit_tf, last20_hit_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-ODF coverage - Top 20%: 0.782, Tail 20%: 0.564\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Set, Tuple\n",
    "def coverage(top_k: Dict[str, List[str]], relevant_items: Set[str]) -> float:\n",
    "\n",
    "    recommended_items = {item for recs in top_k.values() for item, _ in recs}\n",
    "    matched = recommended_items & relevant_items\n",
    "    return round(len(matched)/len(relevant_items), 3) if relevant_items else 0\n",
    "\n",
    "def get_item_groups(df_train: pd.DataFrame) -> Tuple[Set[str], Set[str]]:\n",
    "\n",
    "    item_counts = df_train['item_id'].value_counts()\n",
    "    split_idx = int(len(item_counts) * 0.2)\n",
    "    return set(item_counts.head(split_idx).index), set(item_counts.tail(split_idx).index)\n",
    "\n",
    "# Correct usage\n",
    "top_items, tail_items = get_item_groups(train_df)  # Use actual training data\n",
    "\n",
    "# Calculate coverage for different groups\n",
    "def calculate_group_coverage(top_k: Dict[str, List[str]], items: Set[str]) -> float:\n",
    "    return coverage(top_k, items)\n",
    "\n",
    "top20_cov_tf = calculate_group_coverage(top10_tfidf, top_items)\n",
    "last20_cov_tf = calculate_group_coverage(top10_tfidf, tail_items)\n",
    "\n",
    "print(f\"TF-ODF coverage - Top 20%: {top20_cov_tf}, Tail 20%: {last20_cov_tf}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
