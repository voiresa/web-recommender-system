{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create three hybrid recommender systems by combining a collaborative filtering model with a content-based model using the following three\n",
    "strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('prediction_SVD.pickle', 'rb') as file:\n",
    "    prediction_SVD = pickle.load(file)\n",
    "\n",
    "with open('prediction_TFIDF.pickle', 'rb') as file:\n",
    "    prediction_TFIDF = pickle.load(file)\n",
    "\n",
    "train_df = pd.read_pickle(\"train_dataframe.pkl\") \n",
    "test_df = pd.read_pickle(\"test_dataframe.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel combination strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that re-ranks the items by combining\n",
    "the individual rankings from the two models with some aggregation\n",
    "function such as the sum, average, minimum or maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_pred = {}\n",
    "for user_id, items_svd in prediction_SVD.items():\n",
    "    if user_id in prediction_TFIDF:\n",
    "        items_tfidf = prediction_TFIDF[user_id]\n",
    "        pc_pred[user_id] = {}\n",
    "\n",
    "        for item, rating_svd in items_svd.items():\n",
    "            if item in items_tfidf:\n",
    "                rating_tfidf = items_tfidf[item]\n",
    "                avg_rating = (rating_svd + rating_tfidf) / 2\n",
    "                pc_pred[user_id][item] = avg_rating\n",
    "pc_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Dict, Any, List\n",
    "def get_top_k_for_all_users(predictions: Dict[str, Dict[str, float]], k: int) -> defaultdict:\n",
    "   \n",
    "    top_k_recommendations = defaultdict(list)\n",
    "\n",
    "    for user_id, user_predictions in predictions.items():\n",
    "        sorted_items = sorted(user_predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        top_k_items = [(item_id, round(rating, 3)) for item_id, rating in sorted_items[:k]]\n",
    "        \n",
    "        top_k_recommendations[user_id] = top_k_items\n",
    "\n",
    "    return top_k_recommendations\n",
    "\n",
    "top10_parallel = get_top_k_for_all_users(pc_pred, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Switching strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that uses the recommendations from the collabo-\n",
    "rative filtering model for some users and the recommendations from\n",
    "the content-based model for other users chosen by a predefined con-\n",
    "dition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition: user interaction frequency\n",
    "user_interaction_count = train_df.groupby('user_id').size().reset_index(name='interaction_count')\n",
    "sparsity_threshold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_pred = {}\n",
    "for user_id, items_svd in prediction_SVD.items():\n",
    "    sw_pred[user_id] = {}\n",
    "\n",
    "    if user_interaction_count.get(user_id, 0) >= sparsity_threshold:\n",
    "        for item, rating_svd in items_svd.items():\n",
    "            sw_pred[user_id][item] = rating_svd\n",
    "            \n",
    "    else:\n",
    "        if user_id in prediction_TFIDF:\n",
    "            for item, rating_tfidf in prediction_TFIDF[user_id].items():\n",
    "                sw_pred[user_id][item] = rating_tfidf\n",
    "sw_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_switch = get_top_k_for_all_users(sw_pred, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelining (sequential) strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where a level of one model is used as\n",
    "input to the other model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first content based - get >= 3, then collaborative filtering\n",
    "pp_pred = {}\n",
    "\n",
    "for user_id, items_tfidf in prediction_TFIDF.items():\n",
    "    pp_pred[user_id] = {}\n",
    "\n",
    "    for item, rating_tfidf in items_tfidf.items():\n",
    "        if rating_tfidf >= 3:\n",
    "            if user_id in prediction_SVD and item in prediction_SVD[user_id]:\n",
    "                pp_pred[user_id][item] = prediction_SVD[user_id][item]\n",
    "pp_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_pipeline = get_top_k_for_all_users(pp_pred, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import (absolute_import, division, print_function, unicode_literals)\n",
    "from collections import defaultdict\n",
    "from surprise import Dataset\n",
    "\n",
    "test_df['new_label'] = test_df['rating'].apply(lambda x: 1 if x >= 3 else 0)\n",
    "test_df[test_df['new_label'] == 1]\n",
    "\n",
    "def precision_at_k(top_k: Dict[str, List[str]], df_test: pd.DataFrame, k: int) -> Dict[str, float]:\n",
    "    \"\"\"Compute precision at k for each user\n",
    "    Args:\n",
    "        top_k: A dictionary where keys are user ids (str) and values are lists of (item_id, rating_estimation) tuples.\n",
    "        df_test: Pandas DataFrame containing user-item ratings in the test split.\n",
    "        k: The number of recommendations to output for each user.\n",
    "    Returns:\n",
    "        A dictionary where keys are user ids (str) and values are P@k (float) for each user.\n",
    "    \"\"\"\n",
    "    \n",
    "    precisions = defaultdict(float)\n",
    "    \n",
    "    # Only consider relevant items (rating â‰¥ 4.0)\n",
    "    relevant_items = df_test[df_test['new_label'] == 1].groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "    \n",
    "    for user, recommended_items in top_k.items():\n",
    "        recommended_set = {item for item, _ in recommended_items[:k]}  # Take top-k items\n",
    "        \n",
    "        if user in relevant_items:\n",
    "            num_relevant_at_k = len(recommended_set & relevant_items.get(user, set()))  # Intersection count\n",
    "            if k > 0:  # Avoid division by zero\n",
    "                precisions[user] = round(num_relevant_at_k / min(len(recommended_items), k), 3)  # Compute Precision@k\n",
    "\n",
    "    return precisions\n",
    "\n",
    "\n",
    "\n",
    "def mean_average_precision(top_k: Dict[str, List[str]], df_test: pd.DataFrame, k: int) -> float:\n",
    "    \"\"\"Compute mean average precision (MAP@k)\n",
    "    Args:\n",
    "        top_k: A dictionary where keys are user ids (str) and values are lists of (item_id, rating_estimation) tuples.\n",
    "        df_test: Pandas DataFrame containing user-item ratings in the test split.\n",
    "        k: The number of recommendations to output for each user.\n",
    "    Returns:\n",
    "        MAP@k (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    average_precision_users = []\n",
    "    \n",
    "    # Get relevant items per user\n",
    "    relevant_items = df_test[df_test['new_label'] == 1].groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "\n",
    "    for user, recommended_items in top_k.items():\n",
    "        relevant_set = relevant_items.get(user, set())  # Get relevant items, default to empty set\n",
    "        \n",
    "        num_relevant = 0\n",
    "        precision_sum = 0.0\n",
    "        \n",
    "        for i, (item, _) in enumerate(recommended_items[:k]):  # Iterate over top-K items\n",
    "            if item in relevant_set:\n",
    "                num_relevant += 1\n",
    "                precision_sum += num_relevant / (i + 1)  # Precision at each relevant item\n",
    "\n",
    "        # Avoid division by zero\n",
    "        avg_precision = precision_sum / min(len(recommended_items), k) if num_relevant > 0 else 0\n",
    "        average_precision_users.append(avg_precision)\n",
    "\n",
    "    return np.mean(average_precision_users) if average_precision_users else 0.0\n",
    "\n",
    "\n",
    "def mean_reciprocal_rank(top_k: Dict[str, List[str]], df_test: pd.DataFrame, k: int) -> float:\n",
    "    \"\"\"Compute mean reciprocal rank (MRR@k)\n",
    "    Args:\n",
    "        top_k: A dictionary where keys are user ids (str) and values are lists of (item_id, rating_estimation) tuples.\n",
    "        df_test: Pandas DataFrame containing user-item ratings in the test split.\n",
    "        k: The number of recommendations to output for each user.\n",
    "    Returns:\n",
    "        MRR@k (float)\n",
    "    \"\"\"\n",
    "    \n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    # Get relevant items per user\n",
    "    relevant_items = df_test[df_test['new_label'] == 1].groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "\n",
    "    for user, recommended_items in top_k.items():\n",
    "        relevant_set = relevant_items.get(user, set())  # Get relevant items, default to empty set\n",
    "        found_relevant = False\n",
    "        \n",
    "        for i, (item, _) in enumerate(recommended_items[:k]):  # Iterate over top-K items\n",
    "            if item in relevant_set:  # Find first relevant item\n",
    "                reciprocal_ranks.append(1 / (i + 1))\n",
    "                found_relevant = True\n",
    "                break  # Stop after first relevant item\n",
    "\n",
    "        if not found_relevant:\n",
    "            reciprocal_ranks.append(0)  # Assign 0 if no relevant item is found\n",
    "\n",
    "    return np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0\n",
    "\n",
    "\n",
    "\n",
    "def hit_rate(top_k: Dict[str, List[str]],\n",
    "             df_test: pd.DataFrame) -> float:\n",
    "    \"\"\"Compute the hit rate\n",
    "    Args:\n",
    "        top_k: A dictionary where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n (output of get_top_k())\n",
    "        df_test: Pandas DataFrame containing user-item ratings in \n",
    "            the test split.\n",
    "    Returns:\n",
    "        The average hit rate\n",
    "    \"\"\"\n",
    "\n",
    "    hits = 0\n",
    "    # Get relevant items per user\n",
    "    relevant_items = df_test[df_test['new_label'] == 1].groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "    total_users = len(df_test[df_test['new_label'] == 1]['user_id'].unique())\n",
    "\n",
    "    for user, recommended_items in top_k.items():\n",
    "        recommended_set = {item for item, _ in recommended_items}  # Extract recommended item IDs\n",
    "        if user in relevant_items:\n",
    "            if recommended_set & relevant_items[user]:  # Check if there is any intersection\n",
    "                hits += 1  \n",
    "\n",
    "    return round(hits / total_users, 3) if total_users > 0 else 0.0\n",
    "\n",
    "\n",
    "\n",
    "def coverage(top_k: Dict[str, List[str]], df_test: pd.DataFrame, df_train: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Compute catalog coverage.\n",
    "\n",
    "    Args:\n",
    "        top_k: A dictionary where keys are user (raw) ids and values are lists of tuples:\n",
    "               [(raw item id, rating estimation), ...] (output of get_top_k()).\n",
    "        df_test: Pandas DataFrame containing the training data (user-item interactions).\n",
    "\n",
    "    Returns:\n",
    "        Coverage as a float (rounded to 3 decimals).\n",
    "    \"\"\"\n",
    "    if not top_k:\n",
    "        return 0.0  # No recommendations made\n",
    "\n",
    "    recommended_items = {item for recommendations in top_k.values() for item, _ in recommendations}\n",
    "    all_items = set(df_train[\"item_id\"].unique()) | set(df_test[\"item_id\"].unique()) \n",
    "\n",
    "    coverage_score = len(recommended_items) / len(all_items) if all_items else 0\n",
    "\n",
    "    return round(coverage_score, 3)  # Round to 3 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metrics for Parallel Strategy:\")\n",
    "# PRECISION\n",
    "precisions_nb = precision_at_k(top10_parallel, test_df, k=10)\n",
    "print(\"Averaged P@10: {:.3f}\".format(sum(prec for prec in precisions_nb.values()) / len(precisions_nb)))\n",
    "# MAP \n",
    "map_nb = mean_average_precision(top10_parallel, test_df, k=10)\n",
    "print(\"MAP@10: {:.3f}\".format(map_nb))\n",
    "# MRR\n",
    "mrr_nb = mean_reciprocal_rank(top10_parallel, test_df, k=10)\n",
    "print(\"MRR@10: {:.3f}\".format(mrr_nb))\n",
    "# hit rate\n",
    "hit_r = hit_rate(top10_parallel, test_df)\n",
    "print(\"Hit rate@10: {:.3f}\".format(hit_r))\n",
    "# coverage\n",
    "cover = coverage(top10_parallel, test_df, train_df)\n",
    "print(\"Coverage@10: {:.3f}\".format(cover))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metrics for Switch Strategy:\")\n",
    "# PRECISION\n",
    "precisions_nb = precision_at_k(top10_switch, test_df, k=10)\n",
    "print(\"Averaged P@10: {:.3f}\".format(sum(prec for prec in precisions_nb.values()) / len(precisions_nb)))\n",
    "# MAP \n",
    "map_nb = mean_average_precision(top10_switch, test_df, k=10)\n",
    "print(\"MAP@10: {:.3f}\".format(map_nb))\n",
    "# MRR\n",
    "mrr_nb = mean_reciprocal_rank(top10_switch, test_df, k=10)\n",
    "print(\"MRR@10: {:.3f}\".format(mrr_nb))\n",
    "# hit rate\n",
    "hit_r = hit_rate(top10_switch, test_df)\n",
    "print(\"Hit rate@10: {:.3f}\".format(hit_r))\n",
    "# coverage\n",
    "cover = coverage(top10_switch, test_df, train_df)\n",
    "print(\"Coverage@10: {:.3f}\".format(cover))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Metrics for Pipeline Strategy:\")\n",
    "# PRECISION\n",
    "precisions_nb = precision_at_k(top10_pipeline, test_df, k=10)\n",
    "print(\"Averaged P@10: {:.3f}\".format(sum(prec for prec in precisions_nb.values()) / len(precisions_nb)))\n",
    "# MAP \n",
    "map_nb = mean_average_precision(top10_pipeline, test_df, k=10)\n",
    "print(\"MAP@10: {:.3f}\".format(map_nb))\n",
    "# MRR\n",
    "mrr_nb = mean_reciprocal_rank(top10_pipeline, test_df, k=10)\n",
    "print(\"MRR@10: {:.3f}\".format(mrr_nb))\n",
    "# hit rate\n",
    "hit_r = hit_rate(top10_pipeline, test_df)\n",
    "print(\"Hit rate@10: {:.3f}\".format(hit_r))\n",
    "# coverage\n",
    "cover = coverage(top10_pipeline, test_df, train_df)\n",
    "print(\"Coverage@10: {:.3f}\".format(cover))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### long tail analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "def hit_rate(top_k: Dict[str, List[tuple]], df_test: pd.DataFrame) -> float:\n",
    "    hits = 0\n",
    "    relevant_items = df_test[df_test['new_label'] == 1].groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
    "    total_users = len(relevant_items)\n",
    "\n",
    "    for user, recommended_items in top_k.items():\n",
    "        if user in relevant_items:\n",
    "            recommended_set = {item for item, _ in recommended_items}\n",
    "            if recommended_set & relevant_items[user]:\n",
    "                hits += 1\n",
    "\n",
    "    return round(hits / total_users, 3) if total_users > 0 else 0.0\n",
    "\n",
    "def top_and_last_20_hit_rate(top_k: Dict[str, List[tuple]], df_test: pd.DataFrame, df_train: pd.DataFrame) -> Tuple[float, float]:\n",
    "    user_interaction_counts = df_train.groupby('user_id').size().sort_values(ascending=False)\n",
    "    num_users = len(user_interaction_counts)\n",
    "    top_20_users = set(user_interaction_counts.head(int(0.2 * num_users)).index)\n",
    "    last_20_users = set(user_interaction_counts.tail(int(0.2 * num_users)).index)\n",
    "\n",
    "    df_test_top_20 = df_test[df_test['user_id'].isin(top_20_users)]\n",
    "    df_test_last_20 = df_test[df_test['user_id'].isin(last_20_users)]\n",
    "\n",
    "    top_20_hr = hit_rate(top_k, df_test_top_20)\n",
    "    last_20_hr = hit_rate(top_k, df_test_last_20)\n",
    "\n",
    "    return top_20_hr, last_20_hr\n",
    "\n",
    "top20_hit_pc, last20_hit_pc = top_and_last_20_hit_rate(top10_parallel, test_df, train_df)\n",
    "top20_hit_sw, last20_hit_sw = top_and_last_20_hit_rate(top10_switch, test_df, train_df)\n",
    "top20_hit_pp, last20_hit_pp = top_and_last_20_hit_rate(top10_pipeline, test_df, train_df)\n",
    "\n",
    "print(\"Parallel combination Hit Rate - Top 20%: {:.3f}, Last 20%: {:.3f}\".format(top20_hit_pc, last20_hit_pc))\n",
    "print(\"Switching Hit Rate - Top 20%: {:.3f}, Last 20%: {:.3f}\".format(top20_hit_sw, last20_hit_sw))\n",
    "print(\"Pipelining Hit Rate - Top 20%: {:.3f}, Last 20%: {:.3f}\".format(top20_hit_pp, last20_hit_pp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Set, Tuple\n",
    "def coverage(top_k: Dict[str, List[str]], relevant_items: Set[str]) -> float:\n",
    "\n",
    "    recommended_items = {item for recs in top_k.values() for item, _ in recs}\n",
    "    matched = recommended_items & relevant_items\n",
    "    return round(len(matched)/len(relevant_items), 3) if relevant_items else 0\n",
    "\n",
    "def get_item_groups(df_train: pd.DataFrame) -> Tuple[Set[str], Set[str]]:\n",
    "\n",
    "    item_counts = df_train['item_id'].value_counts()\n",
    "    split_idx = int(len(item_counts) * 0.2)\n",
    "    return set(item_counts.head(split_idx).index), set(item_counts.tail(split_idx).index)\n",
    "\n",
    "# Correct usage\n",
    "top_items, tail_items = get_item_groups(train_df)  # Use actual training data\n",
    "\n",
    "# Calculate coverage for different groups\n",
    "def calculate_group_coverage(top_k: Dict[str, List[str]], items: Set[str]) -> float:\n",
    "    return coverage(top_k, items)\n",
    "\n",
    "top20_cov_pc = calculate_group_coverage(top10_parallel, top_items)\n",
    "last20_cov_pc = calculate_group_coverage(top10_parallel, tail_items)\n",
    "\n",
    "print(f\"Parallel  coverage - Top 20%: {top20_cov_pc}, Tail 20%: {last20_cov_pc}\")\n",
    "\n",
    "top20_cov_sw = calculate_group_coverage(top10_switch, top_items)\n",
    "last20_cov_sw = calculate_group_coverage(top10_switch, tail_items)\n",
    "\n",
    "print(f\"Switching  coverage - Top 20%: {top20_cov_sw}, Tail 20%: {last20_cov_sw}\")\n",
    "\n",
    "top20_cov_pp = calculate_group_coverage(top10_pipeline, top_items)\n",
    "last20_cov_tpp = calculate_group_coverage(top10_pipeline, tail_items)\n",
    "\n",
    "print(f\"Pipelining  coverage - Top 20%: {top20_cov_pp}, Tail 20%: {last20_cov_tpp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 20,\n",
    "    'axes.labelsize': 18, \n",
    "    'xtick.labelsize': 14, \n",
    "    'ytick.labelsize': 14,  \n",
    "    'legend.fontsize': 15,  \n",
    "})\n",
    "\n",
    "models = ['KNN', 'SVD','TF-IDF', 'Parallel', 'Switching', 'Pipelining', 'TopPop']\n",
    "top_20_hr = [0.050, 0.100, 0.167, 0.117, 0.167, 0.150, 0.033] \n",
    "last_20_hr = [0.181, 0.181, 0.163, 0.169, 0.163, 0.119, 0.069] \n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35 \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "rects1 = ax.bar(x - width/2, top_20_hr, width, label='Top 20% Hit Rate', color='skyblue')\n",
    "rects2 = ax.bar(x + width/2, last_20_hr, width, label='Last 20% Hit Rate', color='red')\n",
    "\n",
    "ax.set_xlabel('Recommender Systems')\n",
    "ax.set_ylabel('Hit Rate')\n",
    "ax.set_title('Hit Rate Comparison: Top 20% vs Last 20% Users')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models,ha='right')\n",
    "ax.legend()\n",
    "\n",
    "def add_labels(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "add_labels(rects1)\n",
    "add_labels(rects2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 20, \n",
    "    'axes.labelsize': 18,  \n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14, \n",
    "    'legend.fontsize': 15,\n",
    "})\n",
    "\n",
    "models = ['KNN', 'SVD', 'TF-IDF', 'Parallel', 'Switching', 'Pipelining', 'TopPop']\n",
    "top_20_cov = [0.99, 0.277, 0.782, 0.683, 0.782, 0.307, 0.01] \n",
    "tail_20_cov = [0.564, 0.188, 0.564, 0.446, 0.564, 0.238, 0.0] \n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35 \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "rects1 = ax.bar(x - width/2, top_20_cov, width, label='Top 20% Coverage', color='green')\n",
    "rects2 = ax.bar(x + width/2, tail_20_cov, width, label='Tail 20% Coverage', color='orange')\n",
    "\n",
    "ax.set_xlabel('Recommender Systems')\n",
    "ax.set_ylabel('Coverage')\n",
    "ax.set_title('Coverage Comparison: Top 20% vs Tail 20% Items')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, ha='right')\n",
    "ax.legend()\n",
    "\n",
    "def add_labels(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.3f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "add_labels(rects1)\n",
    "add_labels(rects2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
