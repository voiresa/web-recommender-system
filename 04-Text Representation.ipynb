{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Note: for this part, you are free to use all or a subset of the metadata file,\n",
    "e.g., only using items that are in the train/test splits or using the first n\n",
    "characters of the text content. Please justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in metadata: 23984\n",
      "Filtered items: 518\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_train = pd.read_pickle(\"train_dataframe.pkl\")\n",
    "df_test = pd.read_pickle(\"test_dataframe.pkl\")\n",
    "meta_file = 'metadata.tsv'\n",
    "df_meta = pd.read_csv(meta_file, sep='\\t', low_memory=False)\n",
    "print(f\"Total items in metadata: {len(np.unique(df_meta['item_id']))}\")\n",
    "\n",
    "# Only use items in train/splits\n",
    "train_items = set(df_train['item_id'].unique())\n",
    "test_items = set(df_test['item_id'].unique())\n",
    "all_items = train_items.union(test_items)\n",
    "\n",
    "df_meta = df_meta[df_meta['item_id'].isin(all_items)]\n",
    "print(f\"Filtered items: {len(np.unique(df_meta['item_id']))}\")\n",
    "# save filtered dataset\n",
    "df_meta.to_csv('filtered_metadata.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Select the column description from the metadata file and apply ap-\n",
    "propriate preprocessing to clean up the data, for example: tokenization,\n",
    "transformation to lowercase, stopword removal6, or stemming. Motivate\n",
    "your preprocessing choices and report the vocabulary size before and after preprocessing. There are many libraries you can use, including but not\n",
    "limited to, NLTK, spaCy or CoreNLP (requires Java)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Voiresa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Voiresa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered items: 421\n",
      "Vocabulary size before preprocessing: 5996\n",
      "Vocabulary size after preprocessing: 5802\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Select the column description from the metadata file\n",
    "df_meta = df_meta.dropna(subset=['description']) \n",
    "#df_meta = df_meta[df_meta['description'].str.strip() != ''] \n",
    "print(f\"Filtered items: {len(np.unique(df_meta['item_id']))}\")\n",
    "\n",
    "original_vocab = set()\n",
    "for desc in df_meta['description']:\n",
    "    original_vocab.update(word_tokenize(desc.lower()))\n",
    "print(f\"Vocabulary size before preprocessing: {len(original_vocab)}\")\n",
    "\n",
    "# Pre processing\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    " #   stemmer = PorterStemmer()\n",
    " #   tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "df_meta['processed_description'] = df_meta['description'].apply(preprocess_text)\n",
    "\n",
    "processed_vocab = set()\n",
    "for tokens in df_meta['processed_description']:\n",
    "    processed_vocab.update(tokens)\n",
    "print(f\"Vocabulary size after preprocessing: {len(processed_vocab)}\")\n",
    "\n",
    "df_meta.to_csv('preprocessed_metadata.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Represent each item in the TF-IDF vector space. You can use your\n",
    "preferred library for text representation, for example, scikit-learn or\n",
    "gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (421, 5743)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "df_meta = df_meta.reset_index(drop=True)\n",
    "\n",
    "df_meta['processed_description'] = df_meta['processed_description'].apply(\n",
    "    lambda x: ' '.join(x) if isinstance(x, list) else x\n",
    ")\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df_meta['processed_description'])\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Represent each item using pretrained word embeddings (e.g., GloVe, word-\n",
    "2vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader\n",
    "word2vec_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_item_embedding(text, word2vec_vectors):\n",
    "    \"\"\"Compute the average word2vec embedding for text.\"\"\"\n",
    "    tokens = text.split()  # Tokenize \n",
    "    embeddings = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Skip out-of-vocabulary (OOV) tokens\n",
    "        if token in word2vec_vectors:  \n",
    "            embeddings.append(word2vec_vectors[token])\n",
    "    \n",
    "    if not embeddings:\n",
    "        return None\n",
    "    \n",
    "    return np.mean(embeddings, axis=0)  # Average the embeddings\n",
    "\n",
    "# Create a dictionary to store item embeddings\n",
    "item_embeddings = {}\n",
    "for idx, row in df_meta.iterrows():\n",
    "    item_id = row[\"item_id\"]\n",
    "    description = row[\"processed_description\"]\n",
    "    if isinstance(description, list):\n",
    "        description = ' '.join(description)  # Join tokens into a string\n",
    "    embedding = get_item_embedding(description, word2vec_vectors)\n",
    "    embedding = get_item_embedding(description, word2vec_vectors)\n",
    "    if embedding is not None:\n",
    "        item_embeddings[item_id] = embedding\n",
    "len(item_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the similarity between items within the vector spaces by comput-\n",
    "ing their cosine similarity. Compare results obtained with TF-IDF and\n",
    "the word embeddings. Discuss what you find. Note that you can select\n",
    "a subset of items to better highlight the differences between the two text\n",
    "representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF: Cosine similarity matrix : [[1.         0.04214795 0.03344171 ... 0.         0.03967194 0.02087948]\n",
      " [0.04214795 1.         0.         ... 0.         0.         0.        ]\n",
      " [0.03344171 0.         1.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 1.         0.         0.        ]\n",
      " [0.03967194 0.         0.         ... 0.         1.         0.00521881]\n",
      " [0.02087948 0.         0.         ... 0.         0.00521881 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "print(f\"TF-IDF: Cosine similarity matrix : {cosine_sim_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF\n",
      "Cosine similarity between B0B8M5FJ9W and B0002E2G5Q: 0.000000\n",
      "Cosine similarity between B0B8M5FJ9W and B07HGRFG5J: 0.033442\n",
      "Cosine similarity between B0002E2G5Q and B07HGRFG5J: 0.000000\n"
     ]
    }
   ],
   "source": [
    "asin1 = 'B0B8M5FJ9W' \n",
    "asin2 = 'B0002E2G5Q'\n",
    "asin3 = 'B07HGRFG5J'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "index1 = df_meta[df_meta['item_id'] == asin1].index[0]\n",
    "index2 = df_meta[df_meta['item_id'] == asin2].index[0]\n",
    "index3 = df_meta[df_meta['item_id'] == asin3].index[0]\n",
    "\n",
    "vector1 = tfidf_matrix[index1]\n",
    "vector2 = tfidf_matrix[index2]\n",
    "vector3 = tfidf_matrix[index3]\n",
    "\n",
    "similarity_1_2 = cosine_similarity(vector1, vector2)[0][0]\n",
    "similarity_1_3 = cosine_similarity(vector1, vector3)[0][0]\n",
    "similarity_2_3 = cosine_similarity(vector2, vector3)[0][0]\n",
    "\n",
    "print(f\"TF-IDF\")\n",
    "print(f\"Cosine similarity between {asin1} and {asin2}: {similarity_1_2:.6f}\")\n",
    "print(f\"Cosine similarity between {asin1} and {asin3}: {similarity_1_3:.6f}\")\n",
    "print(f\"Cosine similarity between {asin2} and {asin3}: {similarity_2_3:.6f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Cosine Similarity:\n",
      "[[0.9999997  0.62766576 0.5723729  ... 0.52413344 0.7663236  0.74699277]\n",
      " [0.62766576 1.0000002  0.48242587 ... 0.46918514 0.5178212  0.5695845 ]\n",
      " [0.5723729  0.48242587 1.0000001  ... 0.3600452  0.5467724  0.5513881 ]\n",
      " ...\n",
      " [0.52413344 0.46918514 0.3600452  ... 1.0000001  0.46827385 0.43967092]\n",
      " [0.7663236  0.5178212  0.5467724  ... 0.46827385 1.         0.70941985]\n",
      " [0.74699277 0.5695845  0.5513881  ... 0.43967092 0.70941985 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "item_ids = list(item_embeddings.keys())\n",
    "\n",
    "embedding_matrix = np.array([item_embeddings[item_id] for item_id in item_ids])\n",
    "similarity_matrix = cosine_similarity(embedding_matrix)\n",
    "\n",
    "print(\"Word2Vec Cosine Similarity:\")\n",
    "print(similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec\n",
      "Cosine similarity between B0B8M5FJ9W and B0002E2G5Q: 0.331785\n",
      "Cosine similarity between B0B8M5FJ9W and B07HGRFG5J: 0.572373\n",
      "Cosine similarity between B0002E2G5Q and B07HGRFG5J: 0.419767\n"
     ]
    }
   ],
   "source": [
    "embedding1 = item_embeddings.get(asin1)\n",
    "embedding2 = item_embeddings.get(asin2)\n",
    "embedding3 = item_embeddings.get(asin3)\n",
    "\n",
    "def compute_cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
    "    if vec1 is None or vec2 is None:\n",
    "        return None\n",
    "    return cosine_similarity([vec1], [vec2])[0][0]\n",
    "\n",
    "similarity_1_2 = compute_cosine_similarity(embedding1, embedding2)\n",
    "similarity_1_3 = compute_cosine_similarity(embedding1, embedding3)\n",
    "similarity_2_3 = compute_cosine_similarity(embedding2, embedding3)\n",
    "\n",
    "print(f\"Word2Vec\")\n",
    "print(f\"Cosine similarity between {asin1} and {asin2}: {similarity_1_2:.6f}\")\n",
    "print(f\"Cosine similarity between {asin1} and {asin3}: {similarity_1_3:.6f}\")\n",
    "print(f\"Cosine similarity between {asin2} and {asin3}: {similarity_2_3:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. [Optional] Represent each item rated by the users using the word vectors\n",
    "from the last layer of BERT. Here, you can directly use the vectors from\n",
    "the pretrained version of BERT available in huggingface7. To load the\n",
    "model, install the Transformers library and PyTorch8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Vocabulary size of 30522. Input dimension: 768.\n"
     ]
    }
   ],
   "source": [
    "# LOAD TRANSFORMER\n",
    "import torch\n",
    "import transformers\n",
    "assert transformers.__version__ > '4.0.0'\n",
    "\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "\n",
    "# set-up environment\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "modelname = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(modelname)\n",
    "model = BertModel.from_pretrained(modelname).to(DEVICE)\n",
    "\n",
    "# Print out the vocabulary size\n",
    "vocab_size = tokenizer.vocab_size\n",
    "input_dimension = model.config.hidden_size  # This is typically 768 for BERT\n",
    "print(f\"Vocabulary size of {vocab_size}. Input dimension: {input_dimension}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average embeddings shape: torch.Size([421, 768])\n"
     ]
    }
   ],
   "source": [
    "# Represent products in a vector space\n",
    "\n",
    "def batch_encoding(sentences, batch_size=32):\n",
    "    all_inputs = []\n",
    "    all_last_hidden_states = []\n",
    "\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        inputs = {key: value.to(DEVICE) for key, value in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        all_inputs.append(inputs)\n",
    "        all_last_hidden_states.append(last_hidden_states)\n",
    "    \n",
    "    return all_inputs, all_last_hidden_states\n",
    "\n",
    "encoded_inputs, description_last_hidden_states = batch_encoding(df_meta['description'].tolist())\n",
    "\n",
    "average_embeddings = []\n",
    "\n",
    "for i in range(len(description_last_hidden_states)):\n",
    "    for j in range(description_last_hidden_states[i].shape[0]):\n",
    "        hidden_states = description_last_hidden_states[i][j] \n",
    "        mask = encoded_inputs[i]['attention_mask'][j] \n",
    "        \n",
    "        masked_hidden_states = hidden_states[mask == 1] \n",
    "        if masked_hidden_states.size(0) > 0:\n",
    "            avg_embedding = masked_hidden_states.mean(dim=0)\n",
    "        else:\n",
    "            avg_embedding = torch.zeros(model.config.hidden_size).to(DEVICE)\n",
    "        \n",
    "        average_embeddings.append(avg_embedding)\n",
    "\n",
    "average_embeddings_tensor = torch.stack(average_embeddings)\n",
    "\n",
    "print(f\"Average embeddings shape: {average_embeddings_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT： Cosine Similarity:\n",
      "[[1.         0.79508483 0.711087   ... 0.5681307  0.8713387  0.8303369 ]\n",
      " [0.79508483 1.         0.72398275 ... 0.55087686 0.762762   0.7621898 ]\n",
      " [0.711087   0.72398275 1.0000001  ... 0.48726416 0.7183287  0.7013814 ]\n",
      " ...\n",
      " [0.5681307  0.55087686 0.48726416 ... 1.         0.58081186 0.59511703]\n",
      " [0.8713387  0.762762   0.7183287  ... 0.58081186 0.9999999  0.83941567]\n",
      " [0.8303369  0.7621898  0.7013814  ... 0.59511703 0.83941567 1.0000001 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "average_embeddings_np = average_embeddings_tensor.cpu().numpy()\n",
    "\n",
    "cosine_sim_matrix = cosine_similarity(average_embeddings_np)\n",
    "np.set_printoptions(precision=8, suppress=True)\n",
    "\n",
    "print(\"BERT： Cosine Similarity:\")\n",
    "print(cosine_sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT\n",
      "Cosine similarity between B0B8M5FJ9W and B0002E2G5Q: 0.544365\n",
      "Cosine similarity between B0B8M5FJ9W and B07HGRFG5J: 0.711087\n",
      "Cosine similarity between B0002E2G5Q and B07HGRFG5J: 0.573865\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding1 = average_embeddings_tensor[index1].cpu().detach().numpy()\n",
    "embedding2 = average_embeddings_tensor[index2].cpu().detach().numpy()\n",
    "embedding3 = average_embeddings_tensor[index3].cpu().detach().numpy()\n",
    "\n",
    "similarity_1_2 = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "similarity_1_3 = cosine_similarity([embedding1], [embedding3])[0][0]\n",
    "similarity_2_3 = cosine_similarity([embedding2], [embedding3])[0][0]\n",
    "\n",
    "print(\"BERT\")\n",
    "print(f\"Cosine similarity between {asin1} and {asin2}: {similarity_1_2:.6f}\")\n",
    "print(f\"Cosine similarity between {asin1} and {asin3}: {similarity_1_3:.6f}\")\n",
    "print(f\"Cosine similarity between {asin2} and {asin3}: {similarity_2_3:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tfidf.pickle', 'wb') as f:\n",
    "    pickle.dump(tfidf_matrix, f)\n",
    "\n",
    "map_asin_id = {asin: i for i, asin in enumerate(df_meta['item_id'].unique())}\n",
    "with open('map_asin_id.pickle', 'wb') as handle:\n",
    "    pickle.dump(map_asin_id, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
